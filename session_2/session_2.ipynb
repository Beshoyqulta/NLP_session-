{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d1f89a",
   "metadata": {},
   "source": [
    "NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5facd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df868bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131fdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple Inc. was founded by Steve Jobs and is headquartered in California.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690c2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97294eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. → ORG\n",
      "Steve Jobs → PERSON\n",
      "California → GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, \"→\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eef553",
   "metadata": {},
   "source": [
    "Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6e2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cat sat on the mat, The dog ate The cat's food, The cat and the dog are friends.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042de830",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=Counter(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69be7d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 4,\n",
       "         'cat': 3,\n",
       "         'the': 2,\n",
       "         ',': 2,\n",
       "         'dog': 2,\n",
       "         'sat': 1,\n",
       "         'on': 1,\n",
       "         'mat': 1,\n",
       "         'ate': 1,\n",
       "         \"'s\": 1,\n",
       "         'food': 1,\n",
       "         'and': 1,\n",
       "         'are': 1,\n",
       "         'friends': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a563e6c",
   "metadata": {},
   "source": [
    "preprocessing before BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c491175",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"When I first joined Medium, our CEO would occasionally Slack me about decisions I’d made. “I’m curious why…” he’d begin, before mentioning a story I’d curated or post I’d drafted. Honestly, this freaked me out at first. I took “I’m curious” to mean “I’m suspicious” or “I vehemently disagree but am trying to be polite.” It took me a solid year to figure out that, actually, he was literally just curious — and explaining the reasoning behind my decisions made me better at my job. In retrospect, I’m glad I had to do that.\n",
    "\n",
    "On Medium, behavioral researcher and author Maria Keckler, Ph.D., has a similar memory of a colleague (in Keckler’s case it was the VP of Operations at her former company) who led with curiosity.\n",
    "\n",
    "“What struck me most was what she didn’t do,” Keckler writes. “She didn’t jockey for airtime. She didn’t interrupt or grandstand or push her point. […] Influence didn’t have to look like volume or certainty. It could look like curiosity, with an edge of empathy and wisdom.”\n",
    "\n",
    "This is a pattern I’ve noticed among the friends and coworkers I respect most: they’re highly curious, but somehow generous about it, too. They ask clarifying questions so they’re sure they understand. They ask why and usually aren’t satisfied with the first answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26436f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8046f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [t.lower() for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db7d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_simple = Counter(lower_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b827b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('’', 17), ('i', 13), (',', 13), ('.', 13), ('“', 6), ('”', 6), ('me', 5), ('a', 5), ('or', 5), ('to', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(bow_simple.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88d00530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cfdd4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Beshoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a4bb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word for word in word_tokenize(text.lower()) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5955ec64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'cat',\n",
       " 'sat',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mat',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'food',\n",
       " 'the',\n",
       " 'cat',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'are',\n",
       " 'friends']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8424ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = [token for token in tokens if token not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd4848f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'sat', 'mat', 'dog', 'ate', 'cat', 'food', 'cat', 'dog', 'friends']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b581854a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('dog', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd77d25",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a96f2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7400a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "\"The cat sat on the mat\",\n",
    "\" The dog ate the food of the cat\",\n",
    "\" The cat and the dog are friends\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa3d6b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
       " ['the', 'dog', 'ate', 'the', 'food', 'of', 'the', 'cat'],\n",
       " ['the', 'cat', 'and', 'the', 'dog', 'are', 'friends']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize each document using word_tokenize\n",
    "tokenized_docs=[word_tokenize(doc.lower()) for doc in docs if word_tokenize(doc.lower())]\n",
    "tokenized_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d9a2044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0,\n",
       " 'mat': 1,\n",
       " 'sat': 2,\n",
       " 'the': 3,\n",
       " 'ate': 4,\n",
       " 'dog': 5,\n",
       " 'food': 6,\n",
       " 'friends': 7}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "for doc in tokenized_docs:\n",
    "    for word in doc:\n",
    "        if word in stopwords.words('english'):\n",
    "            doc.remove(word)\n",
    "    no_stopwords.append(doc)\n",
    "\n",
    "#Initiate the dictionary function of GENSIM\n",
    "\n",
    "dictionary= Dictionary(no_stopwords)\n",
    "\n",
    "#Check token ids\n",
    "\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4149e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a631a158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (3, 1), (5, 1), (7, 1)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f2d054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate 0.6841916012796777\n",
      "food 0.6841916012796777\n",
      "dog 0.2525147628886298\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create the TFIDF model in gensim\n",
    "tfidf= TfidfModel(corpus)\n",
    "\n",
    "#Print the weights of the tokens in the first document\n",
    "tfidf_weights = tfidf[corpus[1]]\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab31a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
